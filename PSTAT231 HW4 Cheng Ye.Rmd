---
title: "PSTAT231 HW4 Cheng Ye"
author: "Cheng Ye"
date: "2022-11-01"
output: html_document
---

```{r include= FALSE}
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(corrr)
library(corrplot)
library(caret)
library(discrim)
library(poissonreg)
library(klaR)
library(pROC)
tidymodels_prefer()
#Load required packages
```


```{r}
titanic <-read.csv("C:/Cheng Ye/UCSB/PSTAT 231/HW/homework-4/homework-4/data/titanic.csv") %>%
  mutate(survived = factor(survived, 
                           levels = c("Yes", "No")),
         pclass = factor(pclass))
# Loading required dataset
head(titanic) # visualize data set
set.seed(231) # could be any number

```


# Question 1
```{r}
titanic_split <- initial_split(titanic, prop = 0.80, strata = survived) # split the data and stratified on survived,train 80%, test 20%
titanic_train <- training(titanic_split) # training the dataset
titanic_test <- testing(titanic_split) # testing the dataset
dim(titanic_train)
dim(titanic_test)

#Create a recipe identical to the recipe in HW3
titanic_survived_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, titanic_train) %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ starts_with('sex'):fare+age:fare)
summary(titanic_survived_recipe)
```
# Question 2
```{r}
survived_fold = vfold_cv(titanic_train, v=10)
survived_fold

```

# Question 3
```{r}
#We are training the data by spliting it into 10 folds to evaluate the model's ability when given new data.

#The K-fold Cross-Validation is a method we use to estimate skill of machine learning models, the method is that it split the dataset into K number of folds and is used to evaluate the model's ability when given new data.

#Using K-fold Cross-Validation method instead of simple fit the model helps us avoid overfitting and gives the model the opportunity to train on multiple train-test splits.

#If we did use the entire training set the re-sampling method would be LOOCV (leave-one-out cross validation).
```

# Question 4
```{r}

#For logistic regression model 
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_survived_recipe)
log_fit <- fit(log_wkflow, titanic_train)

#For linear discriminant analysis
lda_mod <- discrim_linear() %>% 
  set_engine('MASS') %>% 
  set_mode('classification')
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_survived_recipe)
lda_fit <- fit(lda_wkflow, titanic_train)

#For quadratic discriminant analysis
qda_mod <- discrim_quad() %>% 
  set_engine('MASS') %>% 
  set_mode('classification')
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_survived_recipe)
qda_fit <- fit(qda_wkflow, titanic_train)

```

# Question 5
```{r}
log_fit<-fit_resamples(log_wkflow,survived_fold)
lda_fit<-fit_resamples(lda_wkflow,survived_fold)
qda_fit<-fit_resamples(qda_wkflow,survived_fold)

```

# Question 6
```{r}
log_metrics <- collect_metrics(log_fit)
lda_metrics <- collect_metrics(lda_fit)
qda_metrics <- collect_metrics(qda_fit)

log_metrics
lda_metrics
qda_metrics

#Based on the results, we could observe that the Logistic regression is the best model in this case because logistic regression method has highest accuracy among the three models and the second lowest standard error for accuracy among the three models, hence it is the most accurate in this case

```

# Question 7
```{r}
log_fit_whole<-fit(log_wkflow,titanic_train)
log_fit_whole
```

# Question 8
```{r}
log_prediction <- predict(log_fit_whole, new_data = titanic_test, type = "class")
bind_cols(log_prediction,titanic_test$survived)
train_accuracy <- augment(log_fit_whole, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)
train_accuracy
test_accuracy <- augment(log_fit_whole, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)
test_accuracy

#By observing the results we know that the test accuracy is 0.7374302 while the train accuracy is 0.8286517, so the training accuracy is higher than that of the testing accuracy.
```

# Question 9

$$
\begin{array}{l}
Since\ Y=\beta+\epsilon, \epsilon \sim N(0, \sigma^2), we\ have\ that \\ \operatorname{RSS}(\beta)=\sum_{i=1}^{N}\left(y_{i}-x_{i}^{\top} \beta\right)^{2}, then \\
\operatorname{RSS}(\beta)=\sum_{i=1}^{N}\left(y_{i}-x_{i}^{\top} \beta\right)^{2} = (y-x \beta)^{\top}(y-x \beta) \\By\ differentiating\ it\ with\ respect\ to\ Î², we\ have\ that\\
x^{\top}(y-x \beta)=0 \\
So\ we\ get\ that\hat{\beta}=\left(x^{\top} x\right)^{-1} x^{\top} y
\end{array}
$$


# Question 10
$$
\begin{aligned}
\mathbf{Cov}(\hat{\beta_1},\hat{\beta_2}) &=\left(X^T X\right)^{-1} X^T\left(\sigma^2 I\right)\left(\left(X^T X\right)^{-1} X^T\right)^T =\sigma^2\left(X^T X\right)^{-1} X^T\left(\left(X^T X\right)^{-1} X^T\right)^T=\sigma^2\left(X^T X\right)^{-1} X^T X\left(X^T X\right)^{-1} =\sigma^2\left(X^T X\right)^{-1}
\end{aligned}
$$














